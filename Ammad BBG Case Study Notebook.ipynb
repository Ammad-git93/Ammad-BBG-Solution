{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DecimalType\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"CurrencyConversion\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the sales data from sales_and_traffic_data.csv to euro by\n",
    "using amazon_shop_mapping.csv and the currency conversion api presented or\n",
    "any forex api from your choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the sales data and shop mapping data\n",
    "sales_data = spark.read.csv(\"C:\\Users\\AmmadAnwar\\sales_and_traffic_data.csv\", header=True)\n",
    "shop_mapping_data = spark.read.csv(\"C:\\Users\\AmmadAnwar\\amazon_shop_mapping.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the sales data with shop mapping data to get the currency for each shop\n",
    "joined_data = sales_data.join(shop_mapping_data, on=[\"shop_name\"], how=\"left\")\n",
    "\n",
    "# Define a UDF to perform currency conversion using an API\n",
    "@udf(DecimalType(10, 2))\n",
    "def convert_to_euro(amount, currency):\n",
    "    # Replace with the URL of a currency conversion API\n",
    "    conversion_api_url = \"https://currencylayer.com/\".format(currency, amount)\n",
    "    response = requests.get(conversion_api_url)\n",
    "    if response.status_code == 200:\n",
    "        converted_amount = response.json()[\"result\"]\n",
    "        return converted_amount\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the UDF to convert sales columns to euro\n",
    "for column in [\"ordered_products_sale\", \"ordered_products_sales_b2b\"]:\n",
    "    joined_data = joined_data.withColumn(column + \"_euro\", convert_to_euro(col(column), col(\"currency\")))\n",
    "\n",
    "# Select relevant columns and drop unnecessary ones\n",
    "result_data = joined_data.select(\n",
    "    \"child_asin\", \"sessions\", \"page_views\", \"units_ordered\", \"units_ordered_b2b\",\n",
    "    \"ordered_products_sale_euro\", \"ordered_products_sales_b2b_euro\",\n",
    "    \"total_ordered_items\", \"total_ordered_items_b2b\", \"region\", \"shop_name\", \"report_date\"\n",
    ")\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "result_data.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the total revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"TotalRevenueCalculation\").getOrCreate()\n",
    "\n",
    "# Read the converted sales data\n",
    "sales_data = spark.read.csv(\"converted_sales_data.csv\", header=True)\n",
    "\n",
    "# Calculate the total revenue\n",
    "total_revenue = sales_data.select(\n",
    "    (col(\"ordered_products_sale_euro\") + col(\"ordered_products_sales_b2b_euro\")).alias(\"total_revenue\")\n",
    ").agg({\"total_revenue\": \"sum\"}).collect()[0][0]\n",
    "\n",
    "# Show the total revenue\n",
    "print(\"Total Revenue in Euro:\", total_revenue)\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In separate Dataframes present the total revenue per country and per shop and per\n",
    "month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year, month, sum\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"TotalRevenuePerCountryShopMonth\").getOrCreate()\n",
    "\n",
    "# Read the converted sales data\n",
    "sales_data = spark.read.csv(\"converted_sales_data.csv\", header=True)\n",
    "\n",
    "# Extract country, shop, and month from report_date\n",
    "sales_data = sales_data.withColumn(\"country\", col(\"region\"))\n",
    "sales_data = sales_data.withColumn(\"shop\", col(\"shop_name\"))\n",
    "sales_data = sales_data.withColumn(\"month\", month(col(\"report_date\")))\n",
    "sales_data = sales_data.withColumn(\"year\", year(col(\"report_date\")))\n",
    "\n",
    "# Group by country, shop, and month\n",
    "grouped_data = sales_data.groupBy(\"country\", \"shop\", \"year\", \"month\")\n",
    "\n",
    "# Calculate the total revenue per country, shop, and month\n",
    "total_revenue_per_month = grouped_data.agg(sum(\"ordered_products_sale_euro\").alias(\"total_revenue\"))\n",
    "\n",
    "# Show the result\n",
    "total_revenue_per_month.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replicate the above questions in SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "-- Create a new table to store the converted sales data\n",
    "CREATE TABLE converted_sales_data (\n",
    "    child_asin VARCHAR(255),\n",
    "    sessions INT,\n",
    "    page_views INT,\n",
    "    units_ordered INT,\n",
    "    units_ordered_b2b INT,\n",
    "    ordered_products_sale DECIMAL(10, 2),\n",
    "    ordered_products_sales_b2b DECIMAL(10, 2),\n",
    "    total_ordered_items INT,\n",
    "    total_ordered_items_b2b INT,\n",
    "    region VARCHAR(255),\n",
    "    shop_name VARCHAR(255),\n",
    "    report_date DATE,\n",
    "    currency VARCHAR(255)\n",
    ");\n",
    "\n",
    "-- Insert converted sales data by joining with amazon_shop_mapping\n",
    "INSERT INTO converted_sales_data\n",
    "SELECT\n",
    "    sd.child_asin,\n",
    "    sd.sessions,\n",
    "    sd.page_views,\n",
    "    sd.units_ordered,\n",
    "    sd.units_ordered_b2b,\n",
    "    sd.ordered_products_sale,\n",
    "    sd.ordered_products_sales_b2b,\n",
    "    sd.total_ordered_items,\n",
    "    sd.total_ordered_items_b2b,\n",
    "    sd.region,\n",
    "    sd.shop_name,\n",
    "    sd.report_date,\n",
    "    asm.currency\n",
    "FROM\n",
    "    sales_data sd\n",
    "JOIN\n",
    "    amazon_shop_mapping asm\n",
    "ON\n",
    "    sd.shop_name = asm.shop_name;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the campaign_object.csv by separating the column CREATIVE to\n",
    "multiple columns ( brandName , brandLogoAssetID , headline , asins , brandLogoUrl )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"CampaignObjectTransform\").getOrCreate()\n",
    "\n",
    "# Read the campaign_object.csv file into a DataFrame\n",
    "campaign_df = spark.read.csv(\"path/to/campaign_object.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Define the schema for the CREATIVE column\n",
    "creative_schema = StructType([\n",
    "    StructField(\"brandName\", StringType(), True),\n",
    "    StructField(\"brandLogoAssetID\", StringType(), True),\n",
    "    StructField(\"headline\", StringType(), True),\n",
    "    StructField(\"asins\", ArrayType(StringType()), True),\n",
    "    StructField(\"brandLogoUrl\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Parse the CREATIVE column into a JSON struct\n",
    "campaign_df = campaign_df.withColumn(\"CREATIVE_JSON\", from_json(col(\"CREATIVE\"), creative_schema))\n",
    "\n",
    "# Extract individual columns from the JSON struct\n",
    "campaign_df = campaign_df.withColumn(\"brandName\", col(\"CREATIVE_JSON.brandName\"))\n",
    "campaign_df = campaign_df.withColumn(\"brandLogoAssetID\", col(\"CREATIVE_JSON.brandLogoAssetID\"))\n",
    "campaign_df = campaign_df.withColumn(\"headline\", col(\"CREATIVE_JSON.headline\"))\n",
    "campaign_df = campaign_df.withColumn(\"asins\", col(\"CREATIVE_JSON.asins\"))\n",
    "campaign_df = campaign_df.withColumn(\"brandLogoUrl\", col(\"CREATIVE_JSON.brandLogoUrl\"))\n",
    "\n",
    "# Drop the original CREATIVE and CREATIVE_JSON columns\n",
    "campaign_df = campaign_df.drop(\"CREATIVE\", \"CREATIVE_JSON\")\n",
    "\n",
    "# Show the transformed DataFrame\n",
    "campaign_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate the new formatted column asins to 3 new\n",
    "columns asin_1 , asin_2 , asin_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"SeparateAsins\").getOrCreate()\n",
    "\n",
    "# Read the DataFrame (assuming you've already transformed the campaign_df as described in the previous answer)\n",
    "# campaign_df = ...\n",
    "\n",
    "# Create three new columns asin_1, asin_2, and asin_3 by extracting values from the asins array\n",
    "campaign_df = campaign_df.withColumn(\"asin_1\", col(\"asins\").getItem(0))\n",
    "campaign_df = campaign_df.withColumn(\"asin_2\", col(\"asins\").getItem(1))\n",
    "campaign_df = campaign_df.withColumn(\"asin_3\", col(\"asins\").getItem(2))\n",
    "\n",
    "# Show the updated DataFrame\n",
    "campaign_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using sales_and_traffic_data.csv, extract the distinct asin list and save it into a\n",
    "dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"ExtractDistinctAsins\").getOrCreate()\n",
    "\n",
    "# Read the sales_and_traffic_data.csv file into a DataFrame\n",
    "sales_data_df = spark.read.csv(\"path/to/sales_and_traffic_data.csv\", header=True)\n",
    "\n",
    "# Select the distinct 'child_asin' values and store them in a new DataFrame\n",
    "distinct_asins_df = sales_data_df.select(col(\"child_asin\").alias(\"distinct_asin\")).distinct()\n",
    "\n",
    "# Show the distinct asin values\n",
    "distinct_asins_df.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new column in the new dataframe\n",
    "from campaign_object.csv, called active_asin , containing the first asin that exists\n",
    "in the formatted distinct asin list.\n",
    "- Example: if asin_1 exist in the distinct asin list\n",
    "  then active_asin = asin_1 otherwise a comparison with asin_2 is needed and\n",
    "  if asin_2 does not exist in the distinct asin list, we move to asin_3 comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"CreateActiveAsin\").getOrCreate()\n",
    "\n",
    "# Read the campaign_object.csv file into a DataFrame\n",
    "campaign_data_df = spark.read.csv(\"path/to/campaign_object.csv\", header=True)\n",
    "\n",
    "# Extract the distinct asin values and store them in a list\n",
    "distinct_asin_list = [row.distinct_asin for row in distinct_asins_df.collect()]\n",
    "\n",
    "# Create a function to find the active asin based on the distinct asin list\n",
    "def find_active_asin(asin_1, asin_2, asin_3):\n",
    "    if asin_1 in distinct_asin_list:\n",
    "        return asin_1\n",
    "    elif asin_2 in distinct_asin_list:\n",
    "        return asin_2\n",
    "    elif asin_3 in distinct_asin_list:\n",
    "        return asin_3\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Define a UDF (User-Defined Function) to apply the find_active_asin function\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "find_active_asin_udf = udf(find_active_asin, StringType())\n",
    "\n",
    "# Add the 'active_asin' column to the campaign_data_df\n",
    "campaign_data_df = campaign_data_df.withColumn(\n",
    "    \"active_asin\",\n",
    "    find_active_asin_udf(\n",
    "        col(\"asin_1\"),\n",
    "        col(\"asin_2\"),\n",
    "        col(\"asin_3\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Show the DataFrame with the 'active_asin' column\n",
    "campaign_data_df.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
